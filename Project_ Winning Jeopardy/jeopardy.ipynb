{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Winning Jeopardy\n",
    "\n",
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. I am going to work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help to win.\n",
    "\n",
    "The dataset is named jeopardy.csv and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which can be downloaded [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/).\n",
    "\n",
    "Each row in the dataset represents a single question on a single episode of Jeopardy. Here are explanations of each column:\n",
    "\n",
    "- Show Number -- the Jeopardy episode number of the show this question was in.\n",
    "- Air Date -- the date the episode aired.\n",
    "- Round -- the round of Jeopardy that the question was asked in. Jeopardy has several rounds as each episode progresses.\n",
    "- Category -- the category of the question.\n",
    "- Value -- the number of dollars answering the question correctly is worth.\n",
    "- Question -- the text of the question.\n",
    "- Answer -- the text of the answer.\n",
    "\n",
    "First I am going to read the dataset and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "jeopardy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the column names have spaces in front, I am going to remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns = jeopardy.columns.str.strip()\n",
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a close look at the format of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Show Number  19999 non-null  int64 \n",
      " 1   Air Date     19999 non-null  object\n",
      " 2   Round        19999 non-null  object\n",
      " 3   Category     19999 non-null  object\n",
      " 4   Value        19999 non-null  object\n",
      " 5   Question     19999 non-null  object\n",
      " 6   Answer       19999 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "jeopardy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the columns\n",
    "\n",
    "Before starting the analysis, we need to normalize and fix the datatypes of some columns. I need to lowercase **Question** and **Answer** columns and remove the punctuation. the **Value** column should be numeric and the **Air Date** should be a datetime.\n",
    "\n",
    "First I am going to write a function to get in a string and return that string in lowercase and without punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def normalize(text):\n",
    "    text  = text.lower()\n",
    "    text = re.sub('[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "#test normalize function\n",
    "normalize(\"Hello! How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the normalize function to **Question** and **Answer** columns and save the result in **clean_question** and **clean_answer** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    for the last 8 years of his life galileo was u...\n",
       "1    no 2 1912 olympian football star at carlisle i...\n",
       "2    the city of yuma in this state has a record av...\n",
       "3    in 1963 live on the art linkletter show this c...\n",
       "4    signer of the dec of indep framer of the const...\n",
       "Name: clean_question, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normalize)\n",
    "jeopardy['clean_question'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    copernicus\n",
       "1    jim thorpe\n",
       "2       arizona\n",
       "3     mcdonalds\n",
       "4    john adams\n",
       "Name: clean_answer, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normalize)\n",
    "jeopardy['clean_answer'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To normalize the **Value** column I am going to remove the dollar sign from the beginning, convert it from text to numeric and save the result to a new column called **clean_value**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_value(value):\n",
    "    value = re.sub('[^\\w\\s]', '', value)\n",
    "    try:\n",
    "        value_int = int(value)\n",
    "    except ValueError:\n",
    "        value_int = 0\n",
    "    return value_int\n",
    "# test\n",
    "normalize_value('$200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply normalize_value function to Value column\n",
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Air Date** column should also be datatime to enable us to work with easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the types of all columns especially the new ones again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19999 entries, 0 to 19998\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Show Number     19999 non-null  int64         \n",
      " 1   Air Date        19999 non-null  datetime64[ns]\n",
      " 2   Round           19999 non-null  object        \n",
      " 3   Category        19999 non-null  object        \n",
      " 4   Value           19999 non-null  object        \n",
      " 5   Question        19999 non-null  object        \n",
      " 6   Answer          19999 non-null  object        \n",
      " 7   clean_question  19999 non-null  object        \n",
      " 8   clean_answer    19999 non-null  object        \n",
      " 9   clean_value     19999 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(7)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "jeopardy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study\n",
    "\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "- How often the answer is deducible from the question.\n",
    "- How often new questions are repeats of older questions.\n",
    "\n",
    "To answer the second question I need to figure out how often complex words (> 6 characters) reoccur and for the first question I need to see how many times words in the answer also occur in the question.\n",
    "\n",
    "let's start with the first question. I am going to write a function to calculate for each question the ratio of the number of words in answers that are found in questions. Then I am going to apply it to all of the questions and calculate the average of them. In this function, 'the' is removed from the words that are investigated since in not a valuable word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059001965249777744"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_matches_ratio(row):\n",
    "    answer = row['clean_answer']\n",
    "    question = row['clean_question']\n",
    "    split_answer = answer.split()\n",
    "    split_question = question.split()\n",
    "    match_count = 0\n",
    "    if 'the' in split_answer:\n",
    "        split_answer.remove('the')\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    for item in split_answer:\n",
    "        if item in split_question:\n",
    "            match_count += 1\n",
    "    return match_count/len(split_answer)\n",
    "  \n",
    "jeopardy['answer_in_question'] = jeopardy.apply(count_matches_ratio, axis = 1)  \n",
    "jeopardy['answer_in_question'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On average about 6% of the words of answers are found in the questions. So the chance of deducing the answer from the question is quite low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated questions\n",
    "\n",
    "Let's go through the second question and investigate how often new questions are repeated of older ones. I can not completely answer this question the dataset includes only 10% of the full jeopardy question dataset but I am going to investigate it.\n",
    "\n",
    "I am going to check if the terms with six or more characters in questions have been used previously or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894006357823155"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "jeopardy.sort_values('Air Date', inplace = True)\n",
    "for i, row in jeopardy.iterrows():\n",
    "    split_question = row['clean_question'].split()\n",
    "    split_question = [q for q in split_question if len(q)>= 6]\n",
    "    match_count = 0\n",
    "    for term in split_question:\n",
    "        if term in terms_used:\n",
    "            match_count += 1\n",
    "        terms_used.add(term)\n",
    "    if len(split_question) > 0:\n",
    "        match_count = match_count / len(split_question)\n",
    "    question_overlap.append(match_count)\n",
    "jeopardy['question_overlap'] = question_overlap\n",
    "jeopardy['question_overlap'].mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 69% of the complex words in questions are repeated so it seems studying the past questions can be really helpful to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study questions with high value\n",
    "\n",
    "Let's focus our study on questions that pertain to high value questions instead of low value questions. This is helpful to earn more money.\n",
    "\n",
    "I can actually figure out which terms correspond to high-value questions using a chi-squared test. I'll first need to narrow down the questions into two categories:\n",
    "\n",
    "- Low value -- Any row where Value is less than 800.\n",
    "- High value -- Any row where Value is greater than 800.\n",
    "\n",
    "I'll then be able to loop through each of the terms from **terms_used**, and:\n",
    "\n",
    "- Find the number of low value questions the word occurs in.\n",
    "- Find the number of high value questions the word occurs in.\n",
    "- Find the percentage of questions the word occurs in.\n",
    "- Based on the percentage of questions the word occurs in, find expected counts.\n",
    "- Compute the chi squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "I can then find the words with the biggest differences in usage between high and low value questions, by selecting the words with the highest associated chi-squared values. Doing this for all of the words would take a very long time, so I'll just do it for a small sample now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_value(row):\n",
    "    value = 0\n",
    "    if row['clean_value'] > 800:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "jeopardy['high_value'] = jeopardy.apply(categorize_value, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for i, row in jeopardy.iterrows():\n",
    "        split_question = row['clean_question'].split()\n",
    "        if word in split_question:\n",
    "            if row['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hosted',\n",
       " 'depths',\n",
       " 'coleridge',\n",
       " 'scissorhands',\n",
       " 'bellringing',\n",
       " 'sporting',\n",
       " 'hrefhttpwwwjarchivecommedia20080408_dj_24bjpg',\n",
       " 'schindler',\n",
       " 'narcocorridos',\n",
       " 'confederates']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly pick ten elements of terms_used\n",
    "from random import choice\n",
    "comparison_terms = [choice(list(terms_used)) for i in range(10)]\n",
    "comparison_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 14),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 3),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_expected = []\n",
    "for word in comparison_terms:\n",
    "    observed_expected.append(count_values(word))\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've found the observed counts for a few terms, I can compute the expected counts and the chi-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_value_count = 5734\n",
      "low_value_count = 14265\n"
     ]
    }
   ],
   "source": [
    "high_value_count = sum(jeopardy['high_value'])\n",
    "low_value_count = jeopardy[jeopardy['high_value'] == 0]['high_value'].count()\n",
    "print('high_value_count = {}'.format(high_value_count))\n",
    "print('low_value_count = {}'.format(low_value_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=1.0102851115076668, pvalue=0.314834544813388),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=0.02636443308440769, pvalue=0.871013484688921),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "chi_squared = []\n",
    "for high_count, low_count in observed_expected:\n",
    "    total = high_count + low_count\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([high_count, low_count])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    \n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "    \n",
    "chi_squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above result none of the p values is less than 0.05 so there is no significant difference in usage in high value and low value for these words. Additionally, the frequencies were all except one lower than 5, so the chi-squared test isn't as valid. It would be better to run this test with only terms that have higher frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate non-informative words\n",
    "I am trying to eliminate non-informative words to decrease the size of terms_used so I may be able to run count_values function on more data. First I am going to remove **stopwords**.\n",
    "\n",
    "#### Romve stopwords\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. We would not want these words to take up space in our database, or taking up the valuable processing time.\n",
    "Let's remove these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24470"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24453"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in stop_words:\n",
    "    if word in terms_used:\n",
    "        terms_used.remove(word)\n",
    "len(terms_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove hrefhttp\n",
    "looking at the words in terms_used there are some links which seem not relevant to our project question, so I am going to remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23250"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_sr = pd.Series(list(terms_used))\n",
    "terms_used_sr = terms_used_sr[~terms_used_sr.str.contains('hrefhttp')]\n",
    "len(terms_used_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still 23250 words in terms_used. At this stage, I am going to look at the count_values function and see if I can make it run faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-write count_values function \n",
    "\n",
    "Looking at the count_values function there is a loop that iterates over the whole jeopardy dataset. I am going to replace it with the pandas columns operations to make it faster. To make it easier to understand the result, the new function returns the word as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_faster(word):\n",
    "    high_count = 0\n",
    "    low_count = 0\n",
    "    \n",
    "    #regex pattern to match the whole word only\n",
    "    pattern = r\"\\b{}\\b\".format(word)\n",
    "    high_count = jeopardy[(jeopardy['clean_question'].str.contains(pattern, regex = True)) &\n",
    "                         (jeopardy['high_value'] == 1)]['high_value'].count()\n",
    "    low_count = jeopardy[(jeopardy['clean_question'].str.contains(pattern, regex = True)) &\n",
    "                        (jeopardy['high_value'] == 0)]['high_value'].count()\n",
    "    return word, high_count, low_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to make sure that we get the same result as the count_values function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hosted', 3, 14), ('depths', 0, 1), ('coleridge', 0, 1), ('scissorhands', 0, 2), ('bellringing', 0, 1), ('sporting', 1, 3), ('hrefhttpwwwjarchivecommedia20080408_dj_24bjpg', 1, 0), ('schindler', 0, 1), ('narcocorridos', 1, 0), ('confederates', 0, 2)]\n"
     ]
    }
   ],
   "source": [
    "observed_test = []\n",
    "for word in comparison_terms:\n",
    "    observed_test.append(count_values_faster(word))\n",
    "print(observed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test is passed and the results are the same with higher efficiency.\n",
    "\n",
    "I am going to apply this new function on the all terms_used. It takes time to run completely but it is more applicable than count_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          (fraction, 0, 1)\n",
       "1          (solution, 0, 4)\n",
       "2            (beheld, 1, 1)\n",
       "3            (decide, 1, 2)\n",
       "4            (moored, 0, 1)\n",
       "                ...        \n",
       "24447     (mythology, 4, 6)\n",
       "24448     (candidely, 0, 1)\n",
       "24449        (clinic, 4, 8)\n",
       "24450     (cervantes, 0, 5)\n",
       "24452    (brazilwood, 0, 1)\n",
       "Length: 23250, dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_sr = pd.Series(list(terms_used))\n",
    "frequencies = terms_used_sr.apply(count_values_faster)\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words with higher frequencies\n",
    "\n",
    "To make the chi_squared test valid, let's filter the words with high frequency and run the chio squred test on the top 1000 highest frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "      <th>total_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13421</th>\n",
       "      <td>called</td>\n",
       "      <td>168</td>\n",
       "      <td>346</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20594</th>\n",
       "      <td>country</td>\n",
       "      <td>141</td>\n",
       "      <td>332</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>played</td>\n",
       "      <td>77</td>\n",
       "      <td>212</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11928</th>\n",
       "      <td>became</td>\n",
       "      <td>79</td>\n",
       "      <td>203</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8318</th>\n",
       "      <td>american</td>\n",
       "      <td>77</td>\n",
       "      <td>174</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>physics</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>couple</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19034</th>\n",
       "      <td>stopped</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>reported</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16949</th>\n",
       "      <td>brings</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  high_value  low_value  total_value\n",
       "13421    called         168        346          514\n",
       "20594   country         141        332          473\n",
       "3015     played          77        212          289\n",
       "11928    became          79        203          282\n",
       "8318   american          77        174          251\n",
       "...         ...         ...        ...          ...\n",
       "1399    physics           9          5           14\n",
       "10955    couple           3         11           14\n",
       "19034   stopped           1         13           14\n",
       "7613   reported           2         12           14\n",
       "16949    brings           5          9           14\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_high_frequecies(data, size):\n",
    "    frequencies = pd.DataFrame(data, \n",
    "                               columns = ['word', 'high_value', 'low_value'])\n",
    "    frequencies['total_value'] = frequencies['high_value'] + frequencies['low_value']\n",
    "    frequencies.sort_values('total_value', ascending = False, inplace = True)\n",
    "    return(frequencies.head(size))\n",
    "\n",
    "\n",
    "\n",
    "high_frequecies = get_high_frequecies(list(frequencies),1000)\n",
    "high_frequecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13421    [(called, 4.048305063534577, 0.044215717944225...\n",
       "20594    [(country, 0.29967829483482744, 0.584084171311...\n",
       "3015     [(played, 0.5810990283039111, 0.44588185909193...\n",
       "11928    [(became, 0.05956570730840162, 0.8071836789959...\n",
       "8318     [(american, 0.4938111242657224, 0.482232156839...\n",
       "dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_chi_squared(row):\n",
    "    chi_squared = []\n",
    "    total_prop = row['total_value']/jeopardy.shape[0]\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    observed = np.array([row['high_value'], row['low_value']])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    \n",
    "    chi_value, p_value = chisquare(observed, expected)\n",
    "    \n",
    "    chi_squared.append((row['word'], chi_value, p_value, row['high_value'], row['low_value']))\n",
    "    return chi_squared\n",
    "                         \n",
    "    \n",
    "chi_squared = high_frequecies.apply(calculate_chi_squared, axis = 1)\n",
    "chi_squared.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, I am going to filter the words with the p_values less than 0.05 to figure out which words are significantly different in high value and low value. I am also looking for words with higher frequency in high_value questions rather than low_value ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>chi_squared</th>\n",
       "      <th>p_value</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>monitora</td>\n",
       "      <td>45.947439</td>\n",
       "      <td>1.214686e-11</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>target_blanksarah</td>\n",
       "      <td>24.358972</td>\n",
       "      <td>7.995351e-07</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>target_blankkelly</td>\n",
       "      <td>20.921282</td>\n",
       "      <td>4.785483e-06</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>african</td>\n",
       "      <td>17.283572</td>\n",
       "      <td>3.219584e-05</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>painter</td>\n",
       "      <td>16.941684</td>\n",
       "      <td>3.854581e-05</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>target_blankjimmy</td>\n",
       "      <td>16.114608</td>\n",
       "      <td>5.962236e-05</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>target_blankjon</td>\n",
       "      <td>13.979777</td>\n",
       "      <td>1.847876e-04</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>pulitzer</td>\n",
       "      <td>13.429676</td>\n",
       "      <td>2.476749e-04</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>liquid</td>\n",
       "      <td>12.719123</td>\n",
       "      <td>3.619354e-04</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>example</td>\n",
       "      <td>11.997980</td>\n",
       "      <td>5.325823e-04</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>spirit</td>\n",
       "      <td>11.341071</td>\n",
       "      <td>7.581159e-04</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>andrew</td>\n",
       "      <td>11.049381</td>\n",
       "      <td>8.871680e-04</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>relative</td>\n",
       "      <td>9.954357</td>\n",
       "      <td>1.604691e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>plants</td>\n",
       "      <td>9.954357</td>\n",
       "      <td>1.604691e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>border</td>\n",
       "      <td>9.792563</td>\n",
       "      <td>1.752191e-03</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>process</td>\n",
       "      <td>9.542069</td>\n",
       "      <td>2.008152e-03</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>physics</td>\n",
       "      <td>8.682874</td>\n",
       "      <td>3.212141e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>string</td>\n",
       "      <td>8.057304</td>\n",
       "      <td>4.532057e-03</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>elements</td>\n",
       "      <td>7.198788</td>\n",
       "      <td>7.295282e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>marine</td>\n",
       "      <td>6.779092</td>\n",
       "      <td>9.223181e-03</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>jersey</td>\n",
       "      <td>6.652781</td>\n",
       "      <td>9.900120e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>greece</td>\n",
       "      <td>6.361380</td>\n",
       "      <td>1.166308e-02</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>translated</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>filled</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>window</td>\n",
       "      <td>5.950459</td>\n",
       "      <td>1.471346e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>composed</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>particles</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>colony</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>persian</td>\n",
       "      <td>5.772340</td>\n",
       "      <td>1.628035e-02</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>physicist</td>\n",
       "      <td>5.549240</td>\n",
       "      <td>1.848871e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>freedom</td>\n",
       "      <td>5.333590</td>\n",
       "      <td>2.091826e-02</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>committee</td>\n",
       "      <td>4.896281</td>\n",
       "      <td>2.691459e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>portuguese</td>\n",
       "      <td>4.896281</td>\n",
       "      <td>2.691459e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>describe</td>\n",
       "      <td>4.460992</td>\n",
       "      <td>3.467736e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>nature</td>\n",
       "      <td>4.460992</td>\n",
       "      <td>3.467736e-02</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  chi_squared       p_value  high_value  low_value\n",
       "176           monitora    45.947439  1.214686e-11          35         13\n",
       "77   target_blanksarah    24.358972  7.995351e-07          40         33\n",
       "232  target_blankkelly    20.921282  4.785483e-06          25         16\n",
       "94             african    17.283572  3.219584e-05          35         33\n",
       "504            painter    16.941684  3.854581e-05          16          8\n",
       "162  target_blankjimmy    16.114608  5.962236e-05          28         24\n",
       "224    target_blankjon    13.979777  1.847876e-04          23         19\n",
       "485           pulitzer    13.429676  2.476749e-04          15          9\n",
       "394             liquid    12.719123  3.619354e-04          17         12\n",
       "457            example    11.997980  5.325823e-04          15         10\n",
       "620             spirit    11.341071  7.581159e-04          13          8\n",
       "677             andrew    11.049381  8.871680e-04          12          7\n",
       "566           relative     9.954357  1.604691e-03          13          9\n",
       "547             plants     9.954357  1.604691e-03          13          9\n",
       "301             border     9.792563  1.752191e-03          18         16\n",
       "415            process     9.542069  2.008152e-03          15         12\n",
       "995            physics     8.682874  3.212141e-03           9          5\n",
       "436             string     8.057304  4.532057e-03          14         12\n",
       "932           elements     7.198788  7.295282e-03           9          6\n",
       "658             marine     6.779092  9.223181e-03          11          9\n",
       "473             jersey     6.652781  9.900120e-03          13         12\n",
       "744             greece     6.361380  1.166308e-02          10          8\n",
       "847         translated     5.950459  1.471346e-02           9          7\n",
       "835             filled     5.950459  1.471346e-02           9          7\n",
       "860             window     5.950459  1.471346e-02           9          7\n",
       "595           composed     5.772340  1.628035e-02          11         10\n",
       "582          particles     5.772340  1.628035e-02          11         10\n",
       "586             colony     5.772340  1.628035e-02          11         10\n",
       "621            persian     5.772340  1.628035e-02          11         10\n",
       "949          physicist     5.549240  1.848871e-02           8          6\n",
       "670            freedom     5.333590  2.091826e-02          10          9\n",
       "783          committee     4.896281  2.691459e-02           9          8\n",
       "784         portuguese     4.896281  2.691459e-02           9          8\n",
       "899           describe     4.460992  3.467736e-02           8          7\n",
       "876             nature     4.460992  3.467736e-02           8          7"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [c[0] for c in chi_squared]\n",
    "chi_squared_df = pd.DataFrame([c[0] for c in chi_squared], \n",
    "                              columns = ['word', 'chi_squared', 'p_value', 'high_value', 'low_value'])\n",
    "chi_squared_df = chi_squared_df.sort_values('p_value')\n",
    "chi_squared_df = chi_squared_df[(chi_squared_df['p_value'] < 0.05) & \n",
    "                                (chi_squared_df['high_value'] > chi_squared_df['low_value']) ]\n",
    "chi_squared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, a dataset of Jeopardy questions has been used to figure out some patterns in the questions that could help to win. After exploring I figured out that\n",
    "\n",
    "- On average about 6% of the words of answers are found in the questions. So the chance of deducing the answer from the question is quite low.\n",
    "- About 69% of the complex words in questions are repeated so studying the past questions can be really helpful to win.\n",
    "\n",
    "Then I focused my study on questions that pertain to high value questions instead of low value ones. This is helpful to earn more money. Using chi squared test I have got a list of 35 words with higher usage in high value questions and with a statistically significant difference of usage in high value and low value questions.\n",
    "\n",
    "The next step can be finding the questions with the high value containing these words. These questions can be recommended to study to win."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
